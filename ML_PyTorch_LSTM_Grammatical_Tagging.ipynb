{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grammatical Tagging with LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a Recursive Neural Network (RNN) to determine the major 9 categories of words in a sentence: \n",
    "- noun\n",
    "- verb\n",
    "- article\n",
    "- adjective \n",
    "- preposition \n",
    "- pronoun \n",
    "- adverb \n",
    "- conjunction\n",
    "- interjection\n",
    "\n",
    "As this is a simplified example to experiment with Long Short-Term Memory (LSTM) neural network, it will only uses a subset of the 9 categories.  Secifically juss the following 5 catecories:\n",
    "- noun (N)\n",
    "- verb (V)\n",
    "- article (ART)\n",
    "- adjective (ADJ)\n",
    "- pronoun (PRO)\n",
    "\n",
    "With this we can just can just analyze simple sentences, such as \"I like McDonalds\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version:  3.6.10\n",
      "Torch version:  1.5.0\n"
     ]
    }
   ],
   "source": [
    "import platform\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"Python version: \", platform.python_version())\n",
    "print(\"Torch version: \", torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make up some simple sentences as Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'the': 0, 'cat': 1, 'caught': 2, 'mouse': 3, 'loves': 4, 'cheese': 5, 'dog': 6, 'hates': 7, 'sleeps': 8, 'runs': 9, 'i': 10, 'like': 11, 'you': 12, 'she': 13, 'watches': 14, 'tv': 15}\n",
      "{'N': 0, 'V': 1, 'ART': 2, 'ADJ': 3, 'PRO': 4}\n"
     ]
    }
   ],
   "source": [
    "# Create a list of some simple sentences as training data and the category tags\n",
    "training_sentences = [\n",
    "    (\"The cat caught the mouse\".lower().split(), [\"ART\", \"N\", \"V\", \"ART\", \"N\"]),\n",
    "    (\"The mouse loves cheese\".lower().split(), [\"ART\", \"N\", \"V\", \"N\"]),\n",
    "    (\"The dog hates the cat\".lower().split(), [\"ART\", \"N\", \"V\", \"ART\", \"N\"]),\n",
    "    (\"The dog sleeps\".lower().split(), [\"ART\", \"N\", \"V\"]),\n",
    "    (\"The cat runs\".lower().split(), [\"ART\", \"N\", \"V\"]),\n",
    "    (\"I like cheese\".lower().split(), [\"PRO\", \"N\", \"V\"]),\n",
    "    (\"You like the cat\".lower().split(), [\"PRO\", \"V\", \"ART\", \"N\"]),\n",
    "    (\"She watches TV\".lower().split(), [\"PRO\", \"V\", \"N\"])\n",
    "]\n",
    "\n",
    "# print(training_sentences)\n",
    "\n",
    "# Dictionary to map words to indices\n",
    "wordIndex = {}\n",
    "for sentence, tags in training_sentences:\n",
    "    for word in sentence:\n",
    "        if word not in wordIndex:\n",
    "            wordIndex[word] = len(wordIndex)\n",
    "            \n",
    "print(wordIndex)\n",
    "\n",
    "# Dictionary to map tags to indices\n",
    "tagIndex = {\"N\": 0, \"V\": 1, \"ART\": 2, \"ADJ\": 3, \"PRO\": 4 }\n",
    "print(tagIndex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([10, 11,  5])\n"
     ]
    }
   ],
   "source": [
    "# Convert a sentence to a numerical tensor\n",
    "def sentence2tensor(sentence, wordIndex):\n",
    "    '''Convert a word sentence to numerical tensor'''\n",
    "    indexes = [wordIndex[word] for word in sentence]\n",
    "    indexes = np.array(indexes)\n",
    "    return torch.from_numpy(indexes).type(torch.LongTensor)\n",
    "\n",
    "# Check the the tensor conversion\n",
    "sample_tensor = sentence2tensor(\"I like cheese\".lower().split(), wordIndex)\n",
    "print(sample_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the LSTM Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple LSTM that takes in a sentence broken down to sqeuence of words.  The words in the sentence are all from known words list. The network will predict that categories for the words in the sentence.  The prediction is done by applying softmax to the hidden state of the LSTM.  The first layer of the model is an Embeddeding layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GrammaticalTagger(nn.Module):\n",
    "    \n",
    "    def __init__(self, embedding_dim, hidden_dim, vocabulary_size, tagset_size):\n",
    "        '''Init'''\n",
    "        super(GrammaticalTagger, self).__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # Embedding layer turning words into a specificied size vector\n",
    "        self.word_embeddings = nn.Embedding(vocabulary_size, embedding_dim)\n",
    "        \n",
    "        # LSTM layer takes embedded word vectors as inputs and output hidden states\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
    "        \n",
    "        # Linear layer maps hidden layer into the output layer with the number of tags\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n",
    "        \n",
    "        # Initialize hidden state\n",
    "        self.hidden = self.init_hidden()\n",
    "        \n",
    "    def init_hidden(self):\n",
    "        '''Initialize the hidden state'''\n",
    "        # (number of layers, batch size, hidden_dim)\n",
    "        return (torch.zeros(1, 1, self.hidden_dim), torch.zeros(1, 1, self.hidden_dim))\n",
    "    \n",
    "    def forward(self, sentence):\n",
    "        '''Model feedfoward inference'''\n",
    "        # first create embedded word vectors\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        \n",
    "        # Get Output and hidden states \n",
    "        lstm_out, self.hidden = self.lstm(embeds.view(len(sentence), 1, -1), self.hidden)\n",
    "        \n",
    "        # Get the scores for tags\n",
    "        tag_outputs = self.hidden2tag(lstm_out.view(len(sentence), -1))\n",
    "        tag_scores = F.log_softmax(tag_outputs, dim=1)\n",
    "        \n",
    "        return tag_scores\n",
    "                        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate model and set hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding_dim defines the size of word vectors\n",
    "embeddeding_dim = 6\n",
    "hidden_dim = 6\n",
    "\n",
    "# Instantiate model\n",
    "taggerModel = GrammaticalTagger(embeddeding_dim, hidden_dim, len(wordIndex), len(tagIndex))\n",
    "                                \n",
    "# Define loss function and optimizer\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.SGD(taggerModel.parameters(), lr=0.1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sanity Check\n",
    "\n",
    "Pass a test sentence thru just to check that we get a reasonable response thru forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input_tensor:  tensor([0, 6, 2, 0, 1])\n",
      "tensor([[-1.5739, -1.6331, -2.0575, -1.2742, -1.6606],\n",
      "        [-1.5722, -1.6784, -2.0284, -1.2758, -1.6348],\n",
      "        [-1.6024, -1.6161, -2.0454, -1.2536, -1.6869],\n",
      "        [-1.5750, -1.6239, -2.0569, -1.2741, -1.6695],\n",
      "        [-1.5348, -1.6278, -2.0508, -1.3001, -1.6767]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n"
     ]
    }
   ],
   "source": [
    "test_sentence = \"The dog caught the cat\".lower().split()\n",
    "\n",
    "input_tensor = sentence2tensor(test_sentence, wordIndex)\n",
    "print(\"Input_tensor: \", input_tensor)\n",
    "\n",
    "tag_scores = taggerModel(input_tensor)\n",
    "print(tag_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
